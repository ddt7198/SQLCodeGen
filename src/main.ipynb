{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jinja2\n",
    "from jinja2 import  BaseLoader\n",
    "import os\n",
    "import json\n",
    "from inspect import getmembers, isfunction\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.getcwd()\n",
    "CONFIG_PATH = os.path.join(CURRENT_PATH, \"config.yml\")\n",
    "with open(os.path.join(CONFIG_PATH), 'r') as ymlfile:\n",
    "    cfg = yaml.full_load(ymlfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = cfg['ROOT_PATH']\n",
    "output_folder = cfg['OUTPUT_FOLDER']\n",
    "table_list_df = pd.read_excel(root_path + cfg['TABLE_LIST'])\n",
    "source_column = pd.read_excel(root_path + cfg['SOURCE_COLUMN'])\n",
    "template_list = cfg['TEMPLATE'].keys()\n",
    "code_path = root_path + output_folder['code']\n",
    "metadata_path = root_path + output_folder['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "templateLoader = jinja2.FileSystemLoader(searchpath=os.path.join(root_path, 'template'))\n",
    "env = jinja2.Environment(loader=templateLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrame():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_upper_all_columns(df: DataFrame):\n",
    "    \"\"\"\n",
    "    Trim whitespace from ends of each value across all series in dataframe\n",
    "    \"\"\"\n",
    "    return df.applymap(lambda x: x.strip().upper() if isinstance(x, str) else x)\n",
    "\n",
    "def hash_generator(col_list: dict, load_type: str):\n",
    "    \"\"\"\n",
    "        Generate hash_key or surrogate_key for required table based on load type\n",
    "        Arguments: \n",
    "            col_list: dictionary of column names and other values related to that column\n",
    "            load_type: load type of the table\n",
    "        Returns:\n",
    "            SQL query to generate hash_key or surrogate_key\n",
    "    \"\"\"\n",
    "    hash_list = []\n",
    "    sur_list = []\n",
    "    if load_type != 'SCD2':\n",
    "        for col in col_list:\n",
    "            if col_list[col]['HASH_KEY'] == 'Y':\n",
    "                hash_list.append('UPPER(TRIM(COALESCE([' + col_list[col]['COLUMN_NAME']+ \"], '')))\")\n",
    "        col_str = \"CAST(sha2(CONCAT(\" + \"\\n\\t\\t\\t,'||',\".join(hash_list) + \"), 256) as VARBINARY(32)) as hash_key,\"\n",
    "        return col_str\n",
    "    else:\n",
    "        for col in col_list:\n",
    "            if col_list[col]['HASH_KEY'] == 'Y':\n",
    "                hash_list.append('UPPER(TRIM(COALESCE([' + col_list[col]['COLUMN_NAME']+ \"], '')))\")\n",
    "            if col_list[col]['SURROGATE_KEY'] == 'Y' and col_list[col]['COLUMN_NAME'] not in ['T24_LOAD_DATE', 'EFZ_LOAD_DATE']:\n",
    "                sur_list.append('UPPER(TRIM(COALESCE([' + col_list[col]['COLUMN_NAME']+ \"], '')))\")\n",
    "            if col_list[col]['HASH_KEY'] == 'Y' and col_list[col]['SURROGATE_KEY'] == 'Y':\n",
    "                raise Exception(f\"Column {col_list[col]['COLUMN_NAME']} cannot be used as HASH_KEY and SURROGATE_KEY at the same time. Please check\")\n",
    "        \n",
    "        hash_str = \"CAST(sha2(CONCAT(\" + \"\\n\\t\\t\\t,'||',\".join(hash_list) + \"), 256) as VARBINARY(32)) as hash_key,\"\n",
    "        sur_str = \"CAST(sha2(CONCAT(\" + \"\\n\\t\\t\\t,'||',\".join(sur_list) + \"), 256) as VARBINARY(32)) as surrogate_key,\"\n",
    "        return hash_str + '\\n\\t\\t\\t' + sur_str\n",
    "\n",
    "def data_transformation(column_name: str, data_type: str, data_length: str, nullable: str):\n",
    "    \"\"\"\n",
    "        Generate data transformation queries\n",
    "        Arguments:\n",
    "            column_name: name of the column\n",
    "            data_type: data type of that column\n",
    "            data_length: data length of that column\n",
    "            nullable: indicate if that column is null or not\n",
    "        Returns:\n",
    "            SQL queries for casting data\n",
    "    \"\"\"\n",
    "    if data_type in ['DATE']:\n",
    "        if nullable == 'Y' and column_name not in ['T24_LOAD_DATE', 'EFZ_LOAD_DATE']:\n",
    "            return f\"TO_DATE(ISNULL(NULLIF({column_name}, ''), '1900-01-01'), 'YYYYMMDD') AS {column_name}\"\n",
    "        elif nullable == 'N' and column_name not in ['T24_LOAD_DATE', 'EFZ_LOAD_DATE']:\n",
    "            return f\"TO_DATE({column_name}, 'YYYYMMDD') AS {column_name}\"\n",
    "        elif column_name in ['T24_LOAD_DATE', 'EFZ_LOAD_DATE']:\n",
    "            return f\"TO_DATE({column_name}, 'DD-Mon-YYYY') AS {column_name}\"\n",
    "    \n",
    "    elif data_type in ['VARCHAR']:\n",
    "        if nullable == 'Y':\n",
    "            return f\"CAST(NULLIF({column_name}, '') AS [{data_type}]({data_length})) AS {column_name}\"\n",
    "        else: \n",
    "            return f\"CAST({column_name} AS [{data_type}]({data_length})) AS {column_name}\"\n",
    "    \n",
    "    elif data_type in ['INT', 'BIGINT', 'SMALLINT']:\n",
    "        if nullable == 'Y':\n",
    "            return f\"CAST(ISNULL(NULLIF({column_name}, ''), 0) AS [{data_type}]) AS {column_name}\"\n",
    "        else: \n",
    "            return f\"CAST({column_name} AS [{data_type}]) AS {column_name}\"\n",
    "\n",
    "    elif data_type in ['DECIMAL']:\n",
    "        if nullable == 'Y' and data_length != '<NA>':\n",
    "            return f\"CAST(ISNULL(NULLIF({column_name}, ''), 0.0) AS [{data_type}]({data_length})) AS {column_name}\"\n",
    "        elif nullable == 'Y' and data_length == '<NA>':\n",
    "            return f\"CAST(ISNULL(NULLIF({column_name}, ''), 0.0) AS [{data_type}]) AS {column_name}\"\n",
    "        elif nullable == 'N' and data_length != '<NA>': \n",
    "            return f\"CAST({column_name} AS [{data_type}]({data_length})) AS {column_name}\"\n",
    "        else:\n",
    "            return f\"CAST({column_name} AS [{data_type}]) AS {column_name}\"\n",
    "\n",
    "def switch_dist_style(dist_style: str):\n",
    "    \"\"\"\n",
    "        Switch distribution style of the table\n",
    "        Arguments:\n",
    "            dist_style: distribution type from consolidate file\n",
    "        Returns:\n",
    "            SQL queries for choosing distribution style\n",
    "    \"\"\"\n",
    "    if dist_style.upper() == 'KEY':\n",
    "        return \"\"\"DISTSTYLE KEY\\n\\t\\tDISTKEY(hash_key)\\n\\t\\tSORTKEY(hash_key)\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"DISTSTYLE {dist_style.upper()}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_mapping = {\n",
    "    'hash_generator': hash_generator,\n",
    "    'data_transformation': data_transformation,\n",
    "    'switch_dist_style': switch_dist_style\n",
    "}\n",
    "\n",
    "for function_name, function_object in function_mapping.items():\n",
    "    env.globals[function_name] = function_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json_ddl(field):\n",
    "    \"\"\"\n",
    "        Create JSON from each row of the input DataFrame\n",
    "        Arguments:\n",
    "            field: row of the input DataFrame\n",
    "        Returns:\n",
    "            new column with each field as a dict object\n",
    "    \"\"\"\n",
    "    return \"\"\"\n",
    "    {{\n",
    "        \"COLUMN_NAME\": \"{column_name}\", \n",
    "        \"DATA_TYPE\": \"{column_data_type}\", \n",
    "        \"DATA_LENGTH\": \"{column_data_length}\", \n",
    "        \"NULLABLE\": \"{column_nullable}\", \n",
    "        \"HASH_KEY\": \"{column_hash_key}\", \n",
    "        \"SURROGATE_KEY\": \"{column_surrogate_key}\" \n",
    "    }}\n",
    "    \"\"\".format(\n",
    "        column_name=field['COLUMN_NAME'], \n",
    "        column_data_type=field['TARGET_DATA_TYPE'],\n",
    "        column_data_length=field['TARGET_DATA_LENGTH'], \n",
    "        column_nullable=field['NULLABLE'],\n",
    "        column_hash_key=field['HASH_KEY'],\n",
    "        column_surrogate_key=field['SURROGATE_KEY']\n",
    "    )\n",
    "\n",
    "def adjust_target_length(df: DataFrame):\n",
    "    \"\"\"\n",
    "        Change data length of target columns based on mapping condition\n",
    "        Arguments:\n",
    "            df: merged dataframe from all input excel files\n",
    "        Returns:\n",
    "            Mapped data length for column TARGET_DATA_LENGTH\n",
    "    \"\"\"\n",
    "    # if df['TARGET_DATA_TYPE'] == 'DECIMAL':\n",
    "    #     df['TARGET_DATA_LENGTH'] = f\"{df['SOURCE_DECIMAL_PRECISION']}, {df['SOURCE_DECIMAL_SCALE']}\"\n",
    "    if int(df['TARGET_DATA_LENGTH']) == -1:\n",
    "        df['TARGET_DATA_LENGTH'] = df['DATA_LENGTH']\n",
    "    # must return or the code will break\n",
    "    return df\n",
    "\n",
    "def gen_metadata(source_column: DataFrame, table_list_df: DataFrame, mapping: DataFrame):\n",
    "    \"\"\"\n",
    "        Generate metadata\n",
    "        Arguments:\n",
    "            source_column: list of columns of source table\n",
    "            table_list_df: list of table names\n",
    "            mapping: mapping for source2target\n",
    "        Returns:\n",
    "            metadata dataframe  \n",
    "    \"\"\"\n",
    "    pre_metadata = pd.merge(source_column, table_list_df, on=['SOURCE_SYSTEM', 'SOURCE_SCHEMA', 'TABLE_NAME'], how='inner')\n",
    "    metadata = pd.merge(pre_metadata, mapping, left_on=['DATA_TYPE'], right_on=['SOURCE_DATA_TYPE'], how='inner')\n",
    "    metadata.drop(['No.', 'SOURCE_DATA_TYPE'], axis=1, inplace=True)\n",
    "    metadata['DATA_LENGTH'] = metadata['DATA_LENGTH'].astype(pd.Int64Dtype())\n",
    "    metadata['DATA_TYPE'] = metadata['DATA_TYPE'].apply(lambda x: x.strip().upper())\n",
    "    metadata['TARGET_DATA_TYPE'] = metadata['TARGET_DATA_TYPE'].apply(lambda x: x.strip().upper())\n",
    "    metadata = metadata.apply(adjust_target_length, axis=1)\n",
    "    metadata['JSON'] = metadata.apply(lambda x: gen_json_ddl(x), axis=1)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json_mapping_ddl(input_df: DataFrame, template: str, table_name: str):\n",
    "    \"\"\"\n",
    "        Generate mapped json from mapping dataframe\n",
    "        Arguments:\n",
    "            input_df: dataframe of table with at least one template value is 1 from table_list.xlsx\n",
    "            template: name of required template\n",
    "            table_name: name of required table\n",
    "        Returns:\n",
    "            mapped json file\n",
    "            mapped excel file\n",
    "    \"\"\"\n",
    "    info = input_df[input_df['TABLE_NAME'] == table_name]\n",
    "\n",
    "    # Check required path\n",
    "    table_metadata_path = os.path.join(metadata_path, table_name)\n",
    "    table_code_path = os.path.join(code_path, table_name)\n",
    "    table_template_path = os.path.join(code_path, table_name, template)\n",
    "\n",
    "    check_path = [code_path, table_code_path, metadata_path, table_metadata_path, table_template_path]\n",
    "\n",
    "    for path in check_path:\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "    ddl_json = {}\n",
    "    \n",
    "    # Generate column mapping json\n",
    "    ddl_lst = list(info['JSON'])\n",
    "    col_dict = {}\n",
    "    for i in range(len(ddl_lst)):\n",
    "        col_dict[str(i)] = json.loads(ddl_lst[i])\n",
    "\n",
    "    # Generate keys for json mapping\n",
    "    keys_list = ['SOURCE_SCHEMA', 'TABLE_NAME', 'DATA_SUBJECT', 'SOURCE_SYSTEM', 'LOAD_TYPE', 'DIST_STYLE']\n",
    "    for key in keys_list:\n",
    "        ddl_json[key] = info[key].unique()[0]\n",
    "\n",
    "    ddl_json['COLUMNS'] = col_dict\n",
    "\n",
    "    # Write json data to file\n",
    "    with open(os.path.join(table_template_path, table_name + '.json'), 'w') as f:\n",
    "        json.dump(ddl_json, f, indent=3)\n",
    "    \n",
    "    # write metadata to excel file\n",
    "    modified_info = info.drop(['JSON'], axis=1)\n",
    "    modified_info.to_excel(os.path.join(table_metadata_path, template + '.xlsx'), index=False)\n",
    "\n",
    "def gen_output(code_path: str, template: str, template_path: str, table_name: str):\n",
    "    \"\"\"\n",
    "        Generate output files for the required table\n",
    "        Arguments:\n",
    "            code_path: path of code folder\n",
    "            template: name of required template\n",
    "            template_path: path of template file\n",
    "            table_name: name of required table\n",
    "    \"\"\"\n",
    "    # Get json mapping\n",
    "    with open(os.path.join(code_path, table_name, template, table_name + '.json'), 'r') as f:\n",
    "        ddl_json = json.load(f)\n",
    "\n",
    "    # Get jinja template\n",
    "    with open(template_path, 'r') as f:\n",
    "        content_template = f.read()\n",
    "\n",
    "    # Render jinja template using json mapping\n",
    "    code_ddl = env.from_string(content_template).render(ddl_json)\n",
    "\n",
    "    # Write output to file\n",
    "    with open(os.path.join(code_path, table_name, template, table_name + '.sql'), 'w') as f:\n",
    "        f.write(code_ddl)\n",
    "\n",
    "def execute():\n",
    "    for template in table_list_df.columns[6:]:\n",
    "        if template in template_list:\n",
    "            mapping = pd.read_excel(root_path + cfg['TEMPLATE'][template]['mapping_path'])\n",
    "            metadata = gen_metadata(source_column, table_list_df, mapping)\n",
    "            template_path = root_path + cfg['TEMPLATE'][template]['location']\n",
    "            gen_df = metadata[metadata[template] == 1]\n",
    "            if len(gen_df) > 0:\n",
    "                table_list = list(gen_df['TABLE_NAME'].unique())\n",
    "                for table in table_list:\n",
    "                    print(f\"==========START GENERATING TEMPLATE {template} OF TABLE {table}=============\")\n",
    "                    # Generate json for ddl\n",
    "                    gen_json_mapping_ddl(gen_df, template, table)\n",
    "\n",
    "                    # Generate code for ddl\n",
    "                    gen_output(code_path, template, template_path, table)\n",
    "                    print(f\"==========TEMPLATE {template} OF TABLE {table} HAS BEEN CREATED=============\")\n",
    "                    print(\"*****************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========START GENERATING TEMPLATE TEMP_DDL OF TABLE abc=============\n",
      "==========TEMPLATE TEMP_DDL OF TABLE abc HAS BEEN CREATED=============\n",
      "*****************************************************************************\n",
      "==========START GENERATING TEMPLATE TEMP_DDL OF TABLE account=============\n",
      "==========TEMPLATE TEMP_DDL OF TABLE account HAS BEEN CREATED=============\n",
      "*****************************************************************************\n",
      "==========START GENERATING TEMPLATE TEMP_DDL OF TABLE transferring=============\n",
      "==========TEMPLATE TEMP_DDL OF TABLE transferring HAS BEEN CREATED=============\n",
      "*****************************************************************************\n",
      "==========START GENERATING TEMPLATE FINAL_DDL OF TABLE abc=============\n",
      "==========TEMPLATE FINAL_DDL OF TABLE abc HAS BEEN CREATED=============\n",
      "*****************************************************************************\n",
      "==========START GENERATING TEMPLATE FINAL_DDL OF TABLE account=============\n",
      "==========TEMPLATE FINAL_DDL OF TABLE account HAS BEEN CREATED=============\n",
      "*****************************************************************************\n",
      "==========START GENERATING TEMPLATE FINAL_DDL OF TABLE transferring=============\n",
      "==========TEMPLATE FINAL_DDL OF TABLE transferring HAS BEEN CREATED=============\n",
      "*****************************************************************************\n",
      "==========START GENERATING TEMPLATE TEMP2FINAL_FA OF TABLE abc=============\n",
      "==========TEMPLATE TEMP2FINAL_FA OF TABLE abc HAS BEEN CREATED=============\n",
      "*****************************************************************************\n",
      "==========START GENERATING TEMPLATE TEMP2FINAL_SCD2 OF TABLE account=============\n",
      "==========TEMPLATE TEMP2FINAL_SCD2 OF TABLE account HAS BEEN CREATED=============\n",
      "*****************************************************************************\n",
      "==========START GENERATING TEMPLATE TEMP2FINAL_FU OF TABLE transferring=============\n",
      "==========TEMPLATE TEMP2FINAL_FU OF TABLE transferring HAS BEEN CREATED=============\n",
      "*****************************************************************************\n"
     ]
    }
   ],
   "source": [
    "execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
